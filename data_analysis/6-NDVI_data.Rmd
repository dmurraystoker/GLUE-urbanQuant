---
title: "GLUE-urbanQuant"
subtitle: "NDVI Data"
author: "David Murray-Stoker"
output:
  pdf_document:
    toc: true
    toc_depth: 4
    fig_caption: yes
    latex_engine: xelatex
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(results = "hold", fig.pos = "H", fig.align = "center", out.width = "92.5%")
options(knitr.graphics.error = FALSE)
kableExtra::usepackage_latex("float")
```

```{r Load Packages for R Markdown, include = FALSE}
library(kableExtra)
library(knitr)
```

```{r Load Packages & Data, include = FALSE}
## Load the tidyverse
library(tidyverse)

## Packages for data extraction and management
library(MODISTools)
```


# Load Data

```{r create_df_list Function, echo = TRUE}
#' Creates list with dataframes as elements for all CSVs in inpath
#' 
#' @param inpath Path to directory containing CSVs to load
#' 
#' @return df_list. List of dataframes, each from a different CSV
create_df_list <- function(inpath){
  
  # Get all csv files in inpath
  files <- dir(inpath, pattern = "*.csv")
  
  # Read in all the files, appending the path before the filename
  df_list <- files %>%
    map(~ read_csv(file.path(inpath, .), show_col_types = FALSE))
  
  return(df_list)
  
  }
```

\vspace{10pt}

```{r Create Lists of Dataframes, echo = TRUE}
## Load all GLUE transects as a list
GLUE.transects.data.path <- "data/data_filtered/"
GLUE.transects.df.list   <- create_df_list(GLUE.transects.data.path)

# Load all random points as a list
random.points.data.path <- "data/filtered_random_points/"
random.points.df.list   <- create_df_list(random.points.data.path)

## Load all systematic points as a list
systematic.points.data.path <- "data/filtered_systematic_points/"
systematic.points.df.list   <- create_df_list(systematic.points.data.path)

## Load all random transects as a list
random.transects.data.path <- "data/filtered_random_transects/"
random.transects.df.list   <- create_df_list(random.transects.data.path)
```


# Normalized Difference Vegetation Index (NDVI)

Description goes here...


\newpage
### download_GLUE_transect_NDVI Function

```{r download_points_transects_NDVI Function, echo = TRUE}
#' Description goes here...
#' 
#' 

## Set the function
download_GLUE_transect_NDVI <- function(df, data_out){
	
  # Get city name
	city <- df %>% pull(City) %>% unique()
	
	# Get country name
	country <- df %>% pull(Country) %>% unique()
	
	# Set coordinates and metadata
	MODIS.info <- data.frame(
	  "City"      = df$City,
	  "Latitude"  = df[, 7],
	  "Longitude" = df[, 8],
	  "Start"     = "2015-01-01", 
	  "End"       = "2019-12-31"
	  )
	
	# Set empty dataframe for extracted MODIS data
	MODIS.data <- data.frame()
	
	# Retrieve data from MODIS
	for(j in 1:length(MODIS.info$City)){
		## Download the MODIS land cover data for each site
		MODIS.bands <- mt_subset(
			product   = "MOD13Q1",
		  band      = "250m_16_days_NDVI",
		  lat       = MODIS.info[j, 2],
		  lon       = MODIS.info[j, 3],
		  start     = MODIS.info[j, 4],
		  end       = MODIS.info[j, 5],
		  site_name = MODIS.info[j, 1],
		  internal  = TRUE,
		  progress  = FALSE
			)
		
		## Add MODIS values to the dataframe
		MODIS.data <- MODIS.data %>%
			bind_rows(MODIS.bands)
	}
	
	# Get total, minimum, and maximum site numbers for site IDs
	total.sites   <- rownames(df) %>% as.integer()
	minimum.sites <- min(total.sites)
	maximum.sites <- max(total.sites)
	
	# Set local site ID (115 measurements/site) for aggregating and later analysis
	site.ID <- rep(
		minimum.sites:maximum.sites,
		each = 115 # Change depending on the start and end dates
		)
	
	# Organize the downloaded MODIS data
	NDVI.df <- MODIS.data %>%
		dplyr::select(site, value) %>%
		dplyr::rename(City = site, Raw_NDVI = value)
	
	# Add site ID to the data
	NDVI.df$Site_ID <- site.ID
	
	# Set year string to repeat for later data management
	year.string <- c(
		rep("2015", times = 23),
		rep("2016", times = 23),
		rep("2017", times = 23),
		rep("2018", times = 23),
		rep("2019", times = 23)
		)
	
	# Add Year to later average NDVI over the 5 years
	NDVI.df$Year <- rep(year.string, times = length(total.sites))
	
	# Aggregate mean, minimum, and maximum NDVI per site
	NDVI.full.df <- NDVI.df %>%
		# Summarise by site for each year separately
		group_by(Site_ID, Year) %>%
		summarise(Mean_NDVI = mean(Raw_NDVI, na.rm = TRUE),
							Min_NDVI = min(Raw_NDVI, na.rm = TRUE),
							Max_NDVI = max(Raw_NDVI, na.rm = TRUE),
							.groups = "keep") %>%
		ungroup() %>%
	  # Summarise across the 5 sampling years
		group_by(Site_ID) %>%
		summarise(Mean_NDVI = mean(Mean_NDVI, na.rm = TRUE),
							Min_NDVI = mean(Min_NDVI, na.rm = TRUE),
							Max_NDVI = mean(Max_NDVI, na.rm = TRUE),
							.groups = "keep")
	
	# Add city to the data
	NDVI.full.df$City <- city
	
	# Add country to the data
	NDVI.full.df$Country <- country
	
	# Add a unique identifier to the data
	NDVI.full.df$UID <- paste(NDVI.full.df$City, rownames(NDVI.full.df), sep = ".")
	
	# Reorganize the data
	NDVI.full.df <- NDVI.full.df %>%
		dplyr::select(UID, Country, City, Mean_NDVI, Min_NDVI, Max_NDVI)
	
	# Set the outpath for the NDVI data
	outpath <- paste0(data_out, city, "_NDVI.csv")
	
	# Export the filtered dataframe
	write_csv(NDVI.full.df, file = outpath, col_names = TRUE)

	}
```



\newpage
### download_points_transects_NDVI Function

```{r download_points_transects_NDVI Function, echo = TRUE}
#' Description goes here...
#' 
#' 

## Set the function
download_points_transects_NDVI <- function(df, data_out){
	
  # Get city name
	city <- df %>% pull(City) %>% unique()
	
	# Get country name
	country <- df %>% pull(Country) %>% unique()
	
	# Set coordinates and metadata
	MODIS.info <- data.frame(
		"City"      = df$City,
	  "Latitude"  = df[, 4],
	  "Longitude" = df[, 5],
	  "Start"     = "2015-01-01",
	  "End"       = "2019-12-31" 
		)
	
	# Set empty dataframe for extracted MODIS data
	MODIS.data <- data.frame()
	
	# Retrieve data from MODIS
	for(j in 1:length(MODIS.info$City)){
		## Download the MODIS land cover data for each site
		MODIS.bands <- mt_subset(
			product   = "MOD13Q1",
		  band      = "250m_16_days_NDVI",
		  lat       = MODIS.info[j, 2],
		  lon       = MODIS.info[j, 3],
		  start     = MODIS.info[j, 4],
		  end       = MODIS.info[j, 5],
		  site_name = MODIS.info[j, 1],
		  internal  = TRUE,
		  progress  = FALSE
			)
		
		## Add MODIS values to the dataframe
		MODIS.data <- MODIS.data %>%
			bind_rows(MODIS.bands)
	}
	
	# Get total, minimum, and maximum site numbers for site IDs
	total.sites   <- rownames(df) %>% as.integer()
	minimum.sites <- min(total.sites)
	maximum.sites <- max(total.sites)
	
	# Set local site ID (115 measurements/site) for aggregating and later analysis
	site.ID <- rep(
		minimum.sites:maximum.sites,
		each = 115 # Change depending on the start and end dates
		)
	
	# Organize the downloaded MODIS data
	NDVI.df <- MODIS.data %>%
		dplyr::select(site, value) %>%
		dplyr::rename(City = site, Raw_NDVI = value)
	
	# Add site ID to the data
	NDVI.df$Site_ID <- site.ID
	
	# Set year string to repeat for later data management
	year.string <- c(
		rep("2015", times = 23),
		rep("2016", times = 23),
		rep("2017", times = 23),
		rep("2018", times = 23),
		rep("2019", times = 23)
		)
	
	# Add Year to later average NDVI over the 5 years
	NDVI.df$Year <- rep(year.string, times = length(total.sites))
	
	# Aggregate mean, minimum, and maximum NDVI per site
	NDVI.full.df <- NDVI.df %>%
		# Summarise by site for each year separately
		group_by(Site_ID, Year) %>%
		summarise(Mean_NDVI = mean(Raw_NDVI, na.rm = TRUE),
							Min_NDVI = min(Raw_NDVI, na.rm = TRUE),
							Max_NDVI = max(Raw_NDVI, na.rm = TRUE),
							.groups = "keep") %>%
		ungroup() %>%
	  # Summarise across the 5 sampling years
		group_by(Site_ID) %>%
		summarise(Mean_NDVI = mean(Mean_NDVI, na.rm = TRUE),
							Min_NDVI = mean(Min_NDVI, na.rm = TRUE),
							Max_NDVI = mean(Max_NDVI, na.rm = TRUE),
							.groups = "keep")
	
	# Add city to the data
	NDVI.full.df$City <- city
	
	# Add country to the data
	NDVI.full.df$Country <- country
	
	# Add a unique identifier to the data
	NDVI.full.df$UID <- paste(NDVI.full.df$City, rownames(NDVI.full.df), sep = ".")
	
	# Reorganize the data
	NDVI.full.df <- NDVI.full.df %>%
		dplyr::select(UID, Country, City, Mean_NDVI, Min_NDVI, Max_NDVI)
	
	# Set the outpath for the NDVI data
	outpath <- paste0(data_out, city, "_NDVI.csv")
	
	# Export the filtered dataframe
	write_csv(NDVI.full.df, file = outpath, col_names = TRUE)

	}
```




\newpage
### Download NDVI Data

```{r Download NDVI Data, echo = TRUE, message = FALSE, error = FALSE, results = "hide"}
## Set outpath for downloaded NDVI data
# GLUE sites
outpath_1 <- "data/NDVI_data/GLUE_transects_NDVI/"
# Random points
outpath_2 <- "data/NDVI_data/random_points_NDVI/"
# Systematic points
outpath_3 <- "data/NDVI_data/systematic_points_NDVI/"
# Random transects
outpath_4 <- "data/NDVI_data/random_transects_NDVI/"

## Download NDVI data
# GLUE transects
purrr::walk(
	GLUE.transects.df.list, 
	download_GLUE_transect_NDVI,
	data_out = outpath_1
	)
# Random points
purrr::walk(
	random.points.df.list,
	download_points_transects_NDVI,
	data_out = outpath_2
	)
# Systematic points
purrr::walk(
	systematic.points.df.list,
	download_points_transects_NDVI,
	data_out = outpath_3
	)
# Random transects
purrr::walk(
	random.transects.df.list,
	download_points_transects_NDVI,
	data_out = outpath_4
	)
```




\newpage
# Workspace Information

```{r R Packages, echo = FALSE}
df_session_packages <- devtools::session_info()$packages %>% 
  as.data.frame(.) %>% 
  filter(attached == TRUE) %>% 
  dplyr::select(loadedversion, date) %>% 
  rownames_to_column

colnames(df_session_packages) <- c("Package", "Loaded Version", "Date")

kable(
  df_session_packages, 
  booktabs = TRUE,
  caption = "Packages required for data management and analyses."
  ) %>%
	kable_styling(
		full_width = FALSE,
  	latex_options = c("HOLD_position", "striped")
  	) 
```




