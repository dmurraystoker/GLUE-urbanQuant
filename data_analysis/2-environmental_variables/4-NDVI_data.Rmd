---
title: "GLUE-urbanQuant"
subtitle: "NDVI Data"
author: "David Murray-Stoker"
output:
  pdf_document:
    toc: true
    toc_depth: 4
    fig_caption: yes
    latex_engine: xelatex
always_allow_html: true
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(results = "hold", fig.pos = "H", fig.align = "center", out.width = "92.5%")
options(knitr.graphics.error = FALSE)
kableExtra::usepackage_latex("float")
```

```{r Load Packages for R Markdown, include = FALSE}
library(kableExtra)
library(knitr)
```

```{r Load Packages & Data, include = FALSE}
## Load the tidyverse
library(tidyverse)

## Packages for data extraction and management
library(MODISTools)
```







\newpage
# Load Data

```{r create_df_list Function, echo = TRUE}
#' Creates a list with dataframes as elements for all csv files in data_in
#'
#' @param data_in Path to directory containing csv files to load
#'
#' @return df_list List of dataframes, each from a different csv file

## Set the function
create_df_list <- function(data_in) {
  # Get all csv files in data_in
  files <- dir(data_in, pattern = "*.csv")

  # Read in all the files, appending the path before the filename
  df_list <- files %>%
    map(~ read_csv(file.path(data_in, .), show_col_types = FALSE))

  return(df_list)
}
```

\vspace{10pt}

```{r Create Lists of Dataframes, echo = TRUE}
## Load all GLUE transects as a list
GLUE.transect.data.path <- "data/data_filtered/"
GLUE.transect.df.list <- create_df_list(GLUE.transect.data.path)

## Load all random transects as a list
random.transect.data.path <- "data/filtered_random_transects/"
random.transect.df.list <- create_df_list(random.transect.data.path)

## Load all random points as a list
random.points.data.path <- "data/filtered_random_points/"
random.points.df.list <- create_df_list(random.points.data.path)

## Load all systematic points as a list
systematic.points.data.path <- "data/filtered_systematic_points/"
systematic.points.df.list <- create_df_list(systematic.points.data.path)
```







\newpage
# Normalized Difference Vegetation Index (NDVI)

Description goes here...





\newpage
## download_GLUE_transect_NDVI Function

```{r download_GLUE_transect_NDVI Function, echo = TRUE}
#' Downloads NDVI data for a 5-year period (2015-2019) from MODIS for each
#' site along a GLUE transect. NDVI is calculated in 250-m radius of each site.
#' Data on mean, minimum, and maximum NDVI for each site over a 5-year period
#' are exported as a csv file.
#'
#' @param df Dataframe with city and country identifiers and lat/long coordinates
#'   for each site. Coordinates must be in decimal degrees. Separate df for
#'   each city.
#' @param data_out Directory to export csv files with NDVI data.

## Set the function
download_GLUE_transect_NDVI <- function(df, data_out) {
  # Get city name
  city <- df %>%
    pull(City) %>%
    unique()

  # Get country name
  country <- df %>%
    pull(Country) %>%
    unique()

  # Set coordinates and metadata
  MODIS.info <- data.frame(
    "City"      = df$City,
    "Latitude"  = df[, 7],
    "Longitude" = df[, 8],
    "Start"     = "2015-01-01",
    "End"       = "2019-12-31"
  )

  # Set empty dataframe for extracted MODIS data
  MODIS.data <- data.frame()

  # Retrieve data from MODIS
  for (j in 1:length(MODIS.info$City)) {
    ## Download the MODIS land cover data for each site
    MODIS.bands <- mt_subset(
      product = "MOD13Q1",
      band = "250m_16_days_NDVI",
      lat = MODIS.info[j, 2],
      lon = MODIS.info[j, 3],
      start = MODIS.info[j, 4],
      end = MODIS.info[j, 5],
      site_name = MODIS.info[j, 1],
      internal = TRUE,
      progress = FALSE
    )

    ## Add MODIS values to the dataframe
    MODIS.data <- MODIS.data %>%
      bind_rows(MODIS.bands)
  }

  # Get total, minimum, and maximum site numbers for site IDs
  total.sites <- rownames(df) %>% as.integer()
  minimum.sites <- min(total.sites)
  maximum.sites <- max(total.sites)

  # Set local site ID (115 measurements/site) for aggregating and later analysis
  site.ID <- rep(
    minimum.sites:maximum.sites,
    each = 115
  )

  # Organize the downloaded MODIS data
  NDVI.df <- MODIS.data %>%
    select(site, value) %>%
    rename(City = site, Raw_NDVI = value)

  # Add site ID to the data
  NDVI.df$Site_ID <- site.ID

  # Set year string to repeat for later data management
  year.string <- c(
    rep("2015", times = 23),
    rep("2016", times = 23),
    rep("2017", times = 23),
    rep("2018", times = 23),
    rep("2019", times = 23)
  )

  # Add Year to later average NDVI over the 5 years
  NDVI.df$Year <- rep(year.string, times = length(total.sites))

  # Aggregate mean, minimum, and maximum NDVI per site
  NDVI.full.df <- NDVI.df %>%
    # Summarise by site for each year separately
    group_by(Site_ID, Year) %>%
    summarise(
      Mean_NDVI = mean(Raw_NDVI, na.rm = TRUE),
      Min_NDVI = min(Raw_NDVI, na.rm = TRUE),
      Max_NDVI = max(Raw_NDVI, na.rm = TRUE),
      .groups = "keep"
    ) %>%
    ungroup() %>%
    # Summarise across the 5 sampling years
    group_by(Site_ID) %>%
    summarise(
      Mean_NDVI = mean(Mean_NDVI, na.rm = TRUE),
      Min_NDVI = mean(Min_NDVI, na.rm = TRUE),
      Max_NDVI = mean(Max_NDVI, na.rm = TRUE),
      .groups = "keep"
    ) %>%
    ungroup()

  # Add city to the data
  NDVI.full.df$City <- city

  # Add country to the data
  NDVI.full.df$Country <- country

  # Add a unique identifier to the data
  NDVI.full.df$UID <- paste(NDVI.full.df$City, rownames(NDVI.full.df), sep = ".")

  # Reorganize the data
  NDVI.full.df <- NDVI.full.df %>%
    select(UID, Country, City, Mean_NDVI, Min_NDVI, Max_NDVI)

  # Set the data_out for the NDVI data
  data_out <- paste0(data_out, city, "_NDVI.csv")

  # Export the filtered dataframe
  write_csv(NDVI.full.df, file = data_out, col_names = TRUE)
}
```





\newpage
## download_points_transect_NDVI Function

```{r download_points_transect_NDVI Function, echo = TRUE}
#' Downloads NDVI data for a 5-year period (2015-2019) from MODIS for each
#' site based on the random transect, random points, or systematic points 
#' sampling designs. NDVI is calculated in 250-m radius of each site. Data 
#' on mean, minimum, and maximum NDVI for each site over a 5-year period
#' are exported as a csv file.
#'
#' @param df Dataframe with city and country identifiers and lat/long coordinates
#'   for each site. Coordinates must be in decimal degrees. Separate df for
#'   each city.
#' @param data_out Directory to export csv files with NDVI data.

## Set the function
download_points_transect_NDVI <- function(df, data_out) {
  # Get city name
  city <- df %>%
    pull(City) %>%
    unique()

  # Get country name
  country <- df %>%
    pull(Country) %>%
    unique()

  # Set coordinates and metadata
  MODIS.info <- data.frame(
    "City" = df$City,
    "Latitude" = df[, 4],
    "Longitude" = df[, 5],
    "Start" = "2015-01-01",
    "End" = "2019-12-31"
  )

  # Set empty dataframe for extracted MODIS data
  MODIS.data <- data.frame()

  # Retrieve data from MODIS
  for (j in 1:length(MODIS.info$City)) {
    ## Download the MODIS land cover data for each site
    MODIS.bands <- mt_subset(
      product = "MOD13Q1",
      band = "250m_16_days_NDVI",
      lat = MODIS.info[j, 2],
      lon = MODIS.info[j, 3],
      start = MODIS.info[j, 4],
      end = MODIS.info[j, 5],
      site_name = MODIS.info[j, 1],
      internal = TRUE,
      progress = FALSE
    )

    ## Add MODIS values to the dataframe
    MODIS.data <- MODIS.data %>%
      bind_rows(MODIS.bands)
  }

  # Get total, minimum, and maximum site numbers for site IDs
  total.sites <- rownames(df) %>% as.integer()
  minimum.sites <- min(total.sites)
  maximum.sites <- max(total.sites)

  # Set local site ID (115 measurements/site) for aggregating and later analysis
  site.ID <- rep(
    minimum.sites:maximum.sites,
    each = 115
  )

  # Organize the downloaded MODIS data
  NDVI.df <- MODIS.data %>%
    select(site, value) %>%
    rename(City = site, Raw_NDVI = value)

  # Add site ID to the data
  NDVI.df$Site_ID <- site.ID

  # Set year string to repeat for later data management
  year.string <- c(
    rep("2015", times = 23),
    rep("2016", times = 23),
    rep("2017", times = 23),
    rep("2018", times = 23),
    rep("2019", times = 23)
  )

  # Add Year to later average NDVI over the 5 years
  NDVI.df$Year <- rep(year.string, times = length(total.sites))

  # Aggregate mean, minimum, and maximum NDVI per site
  NDVI.full.df <- NDVI.df %>%
    # Summarise by site for each year separately
    group_by(Site_ID, Year) %>%
    summarise(
      Mean_NDVI = mean(Raw_NDVI, na.rm = TRUE),
      Min_NDVI = min(Raw_NDVI, na.rm = TRUE),
      Max_NDVI = max(Raw_NDVI, na.rm = TRUE),
      .groups = "keep"
    ) %>%
    ungroup() %>%
    # Summarise across the 5 sampling years
    group_by(Site_ID) %>%
    summarise(
      Mean_NDVI = mean(Mean_NDVI, na.rm = TRUE),
      Min_NDVI = mean(Min_NDVI, na.rm = TRUE),
      Max_NDVI = mean(Max_NDVI, na.rm = TRUE),
      .groups = "keep"
    ) %>%
    ungroup()

  # Add city to the data
  NDVI.full.df$City <- city

  # Add country to the data
  NDVI.full.df$Country <- country

  # Add a unique identifier to the data
  NDVI.full.df$UID <- paste(NDVI.full.df$City, rownames(NDVI.full.df), sep = ".")

  # Reorganize the data
  NDVI.full.df <- NDVI.full.df %>%
    select(UID, Country, City, Mean_NDVI, Min_NDVI, Max_NDVI)

  # Set the data_out for the NDVI data
  data_out <- paste0(data_out, city, "_NDVI.csv")

  # Export the filtered dataframe
  write_csv(NDVI.full.df, file = data_out, col_names = TRUE)
}
```







\newpage
# Download NDVI Data

```{r Download NDVI Data, echo = TRUE, message = FALSE, error = FALSE, results = "hide"}
## Set data_out for downloaded NDVI data
# GLUE transect
data_out_1 <- "data/NDVI_data/GLUE_transects_NDVI/"
# Random transect
data_out_2 <- "data/NDVI_data/random_transects_NDVI/"
# Random points
data_out_3 <- "data/NDVI_data/random_points_NDVI/"
# Systematic points
data_out_4 <- "data/NDVI_data/systematic_points_NDVI/"

## Download NDVI data
# GLUE transects
purrr::walk(
  GLUE.transect.df.list,
  download_GLUE_transect_NDVI,
  data_out = data_out_1
)
# Random points
purrr::walk(
  random.points.df.list,
  download_points_transect_NDVI,
  data_out = data_out_2
)
# Systematic points
purrr::walk(
  systematic.points.df.list,
  download_points_transect_NDVI,
  data_out = data_out_3
)
# Random transects
purrr::walk(
  random.transect.df.list,
  download_points_transect_NDVI,
  data_out = data_out_4
)
```







\newpage
# Workspace Information

```{r R Packages, echo = FALSE}
df_session_packages <- devtools::session_info()$packages %>%
  as.data.frame(.) %>%
  filter(attached == TRUE) %>%
  dplyr::select(loadedversion, date) %>%
  rownames_to_column()

colnames(df_session_packages) <- c("Package", "Loaded Version", "Date")

kable(
  df_session_packages,
  booktabs = TRUE,
  caption = "Packages required for data management and analyses."
) %>%
  kable_styling(
    full_width = FALSE,
    latex_options = c("HOLD_position", "striped")
  )
```
